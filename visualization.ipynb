{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpfQT3ghGlP6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "import numpy\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import torchvision\n",
        "\n",
        "from torch import nn\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZPr_aW9Hutb",
        "outputId": "0544d02d-996f-455d-f268-5fb8d86aca90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils"
      ],
      "metadata": {
        "id": "ToCSl2luVXM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "id": "dpzwV-LEHySI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Colorization(nn.Module):\n",
        "    def __init__(self, depth_after_fusion):\n",
        "        super().__init__()\n",
        "        self.encoder = _build_encoder()\n",
        "        self.fusion = FusionLayer()\n",
        "        # self.dense = _build_dense()\n",
        "        self.after_fusion = nn.Conv2d(1000+depth_after_fusion, depth_after_fusion, kernel_size = 1)\n",
        "        # self.after_fusion = Conv2D(depth_after_fusion, (1, 1), activation=\"relu\")\n",
        "        self.decoder = _build_decoder(depth_after_fusion)\n",
        "\n",
        "    def forward(self, img_l, vgg):\n",
        "        img_enc = self.encoder(img_l)\n",
        "        img_ab = torch.cat((img_l,img_l,img_l),1)\n",
        "        img_emb = vgg(img_ab)\n",
        "        fusion = self.fusion([img_enc, img_emb])\n",
        "        fusion = self.after_fusion(fusion)\n",
        "        return self.decoder(fusion)\n",
        "\n",
        "def _build_encoder():\n",
        "    model = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def _build_decoder(encoding_depth):\n",
        "    model = nn.Sequential(\n",
        "            nn.Conv2d(encoding_depth, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(32, 3, kernel_size=3, padding=1),\n",
        "            nn.Tanh(),\n",
        "            nn.Upsample(scale_factor=2, mode='nearest')\n",
        "        )\n",
        "    return model\n",
        "\n",
        "def Vgg():\n",
        "    model = torchvision.models.vgg16(weights=\"DEFAULT\") # must use this in order to run on Department machines.\n",
        "    # model = torchvision.models.vgg16(pretrained=True) # must use this in order to run on Department machines.\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    return model\n",
        "\n",
        "class FusionLayer(nn.Module):\n",
        "    def forward(self, inputs, mask=None):\n",
        "        #check !!!\n",
        "        imgs, embs = inputs # [16,256,28,28], [16,1000]\n",
        "        (b,c,h,w) = imgs.shape # (batch_size,256,28,28)\n",
        "        l = embs.shape[1]\n",
        "        embs = embs.unsqueeze(-1).unsqueeze(-1)\n",
        "        embs = embs.expand(b, l, h, w)\n",
        "        output = torch.cat ((imgs,embs),1)\n",
        "        return output\n",
        "\n",
        "    def compute_output_shape(self, input_shapes):\n",
        "        # Must have 2 tensors as input\n",
        "        assert input_shapes and len(input_shapes) == 2\n",
        "        imgs_shape, embs_shape = input_shapes\n",
        "\n",
        "        # The batch size of the two tensors must match\n",
        "        assert imgs_shape[0] == embs_shape[0]\n",
        "\n",
        "        # (batch_size, width, height, embedding_len + depth)\n",
        "        return imgs_shape[:3] + (imgs_shape[3] + embs_shape[1],)\n",
        "\n",
        "# Unet itself could be trained to colorize images\n",
        "class Unet(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Initialize layers\n",
        "        self.encoder0 = nn.Sequential(\n",
        "                            nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, stride=1, padding='same'), \n",
        "                            nn.BatchNorm2d(64), \n",
        "                            nn.LeakyReLU(0.2), \n",
        "                            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding='same'),\n",
        "                            nn.BatchNorm2d(64), \n",
        "                            nn.LeakyReLU(0.2)\n",
        "                        )\n",
        "        self.encoder1 = nn.Sequential(\n",
        "                            nn.MaxPool2d(kernel_size=2, stride=2), \n",
        "                            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding='same'), \n",
        "                            nn.BatchNorm2d(128), \n",
        "                            nn.LeakyReLU(0.2), \n",
        "                            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding='same'),\n",
        "                            nn.BatchNorm2d(128), \n",
        "                            nn.LeakyReLU(0.2)\n",
        "                        )\n",
        "        self.encoder2 = nn.Sequential(\n",
        "                            nn.MaxPool2d(kernel_size=2, stride=2), \n",
        "                            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding='same'), \n",
        "                            nn.BatchNorm2d(256), \n",
        "                            nn.LeakyReLU(0.2), \n",
        "                            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding='same'),\n",
        "                            nn.BatchNorm2d(256), \n",
        "                            nn.LeakyReLU(0.2)\n",
        "                        )\n",
        "        self.encoder3 = nn.Sequential(\n",
        "                            nn.MaxPool2d(kernel_size=2, stride=2), \n",
        "                            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding='same'), \n",
        "                            nn.BatchNorm2d(512), \n",
        "                            nn.LeakyReLU(0.2), \n",
        "                            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding='same'),\n",
        "                            nn.BatchNorm2d(512), \n",
        "                            nn.LeakyReLU(0.2)\n",
        "                        )\n",
        "        self.encoder4 = nn.Sequential(\n",
        "                            nn.MaxPool2d(kernel_size=2, stride=2), \n",
        "                            nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=3, stride=1, padding='same'), \n",
        "                            nn.BatchNorm2d(1024), \n",
        "                            nn.LeakyReLU(0.2), \n",
        "                            nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=3, stride=1, padding='same'),\n",
        "                            nn.BatchNorm2d(1024), \n",
        "                            nn.LeakyReLU(0.2)\n",
        "                        )\n",
        "        self.up_sample4 = nn.ConvTranspose2d(in_channels=1024, out_channels=512, kernel_size=2, stride=2)\n",
        "        self.conv4 = nn.Sequential(\n",
        "                        nn.Conv2d(in_channels=1024, out_channels=512, kernel_size=3, stride=1, padding='same'), \n",
        "                        nn.BatchNorm2d(512), \n",
        "                        nn.ReLU(0.2), \n",
        "                        nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding='same'),\n",
        "                        nn.BatchNorm2d(512), \n",
        "                        nn.ReLU(0.2)\n",
        "                    )\n",
        "        self.up_sample3 = nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=2, stride=2)\n",
        "        self.conv3 = nn.Sequential(\n",
        "                        nn.Conv2d(in_channels=512, out_channels=256, kernel_size=3, stride=1, padding='same'), \n",
        "                        nn.BatchNorm2d(256), \n",
        "                        nn.ReLU(0.2), \n",
        "                        nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding='same'),\n",
        "                        nn.BatchNorm2d(256), \n",
        "                        nn.ReLU(0.2)\n",
        "                    )\n",
        "        self.up_sample2 = nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Sequential(\n",
        "                        nn.Conv2d(in_channels=256, out_channels=128, kernel_size=3, stride=1, padding='same'), \n",
        "                        nn.BatchNorm2d(128), \n",
        "                        nn.ReLU(0.2), \n",
        "                        nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding='same'),\n",
        "                        nn.BatchNorm2d(128), \n",
        "                        nn.ReLU(0.2)\n",
        "                    )\n",
        "        self.up_sample1 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=2, stride=2)\n",
        "        self.conv1 = nn.Sequential(\n",
        "                        nn.Conv2d(in_channels=128, out_channels=64, kernel_size=3, stride=1, padding='same'), \n",
        "                        nn.BatchNorm2d(64), \n",
        "                        nn.ReLU(0.2), \n",
        "                        nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding='same'),\n",
        "                        nn.BatchNorm2d(64), \n",
        "                        nn.ReLU(0.2)\n",
        "                    )\n",
        "        self.conv0 = nn.Conv2d(in_channels=64, out_channels=3, kernel_size=1, stride=1)\n",
        "        \n",
        "    \n",
        "    def forward(self, x):\n",
        "        # forward pass of the model\n",
        "        # denote x as input grayscale image\n",
        "        # returns y_pred as predicted color image\n",
        "        x0 = self.encoder0(x) # shape [B, 64, H, W]\n",
        "        x1 = self.encoder1(x0) # shape [B, 128, H/2, W/2]\n",
        "        x2 = self.encoder2(x1) # shape [B, 256, H/4, W/4]\n",
        "        x3 = self.encoder3(x2) # shape [B, 512, H/8, W/8]\n",
        "        x4 = self.encoder4(x3) # shape [B, 1024, H/16, W/16]\n",
        "        x4 = self.up_sample4(x4) # shape [B, 512, H/8, W/8]\n",
        "        x3 = self.conv4(torch.cat((x3, x4), 1)) # shape [B, 512, H/8, W/8]\n",
        "        x3 = self.up_sample3(x3) # shape [B, 256, H/4, W/4]\n",
        "        x2 = self.conv3(torch.cat((x2, x3), 1)) # shape [B, 256, H/4, W/4]\n",
        "        x2 = self.up_sample2(x2) # shape [B, 128, H/2, W/2]\n",
        "        x1 = self.conv2(torch.cat((x1, x2), 1)) # shape [B, 128, H/2, W/2]\n",
        "        x1 = self.up_sample1(x1) # shape [B, 64, H, W]\n",
        "        x0 = self.conv1(torch.cat((x0, x1), 1)) # shape [B, 64, H, W]\n",
        "        return self.conv0(x0)\n",
        "\n",
        "#########################################################################################################################\n",
        "# Conditional GAN, where the generator is the Unet defined above\n",
        "#########################################################################################################################\n",
        "class Dis(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Initialize layers\n",
        "        self.block1 = nn.Sequential(\n",
        "                            nn.Conv2d(in_channels=4, out_channels=64, kernel_size=4, stride=2, padding=(1,1)), \n",
        "                            nn.LeakyReLU(0.2, inplace=True) \n",
        "                        )\n",
        "        self.block2 = nn.Sequential(\n",
        "                            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=4, stride=2, padding=(1,1), bias=False), \n",
        "                            nn.BatchNorm2d(128), \n",
        "                            nn.LeakyReLU(0.2, inplace=True) \n",
        "                        )\n",
        "        self.block3 = nn.Sequential(\n",
        "                            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=4, stride=2, padding=(1,1), bias=False), \n",
        "                            nn.BatchNorm2d(256), \n",
        "                            nn.LeakyReLU(0.2, inplace=True) \n",
        "                        )\n",
        "        self.block4 = nn.Sequential(\n",
        "                            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=4, stride=1, padding=(1,1), bias=False), \n",
        "                            nn.BatchNorm2d(512), \n",
        "                            nn.LeakyReLU(0.2, inplace=True) \n",
        "                        )\n",
        "        self.block5 = nn.Conv2d(in_channels=512, out_channels=1, kernel_size=4, stride=1, padding=(1,1))\n",
        "        self.dense = nn.Sequential(nn.Flatten(), \n",
        "                                   nn.Linear(in_features=676, out_features=15),\n",
        "                                   nn.Linear(in_features=15, out_features=1), \n",
        "                                   nn.Sigmoid())\n",
        "\n",
        "    \n",
        "    def forward(self, img_ab, img_l):\n",
        "        img_comb = torch.cat((img_ab, img_l), dim=1)\n",
        "        layers = [self.block1, self.block2, self.block3, self.block4, self.block5]\n",
        "        for layer in layers:\n",
        "          img_comb = layer(img_comb)\n",
        "        img_comb = self.dense(img_comb)\n",
        "        return img_comb\n",
        "\n",
        "class CGAN(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.lda = 1e-2 # hyperparameter for l2 loss\n",
        "        self.buffer = 1e-2\n",
        "        self.net_G = Unet()\n",
        "        self.net_D = Dis()\n",
        "\n",
        "    def forward(self, imgs_l, imgs_ab):\n",
        "        fake_color = self.net_G(imgs_l)\n",
        "        fake_prob = self.net_D(fake_color, imgs_l)\n",
        "        true_prob = self.net_D(imgs_ab, imgs_l)\n",
        "        return fake_prob, true_prob, fake_color\n",
        "\n",
        "    def loss(self, fake_prob, true_prob, fake_color, imgs_ab):\n",
        "        # fake_prob and true_prob are both of shape [B, 1]\n",
        "        gan_loss = torch.mean(torch.log(true_prob + self.buffer) + torch.log(1 - fake_prob + self.buffer))\n",
        "        l1loss = nn.functional.l1_loss(fake_color, imgs_ab)\n",
        "        print(l1loss)\n",
        "        return gan_loss + l1loss\n"
      ],
      "metadata": {
        "id": "4UCZzVekHwPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "oqud34uEQq0R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset1(Dataset):\n",
        "    def __init__(self, gray_dir, color_dir, transform, testing=False):\n",
        "        # gray_dir: directory of grayscale images\n",
        "        # color_dir: directory of color images\n",
        "        # transform: the possible transformation\n",
        "        # out_size: size of output image\n",
        "        self.gray_dir = gray_dir\n",
        "        self.color_dir = color_dir\n",
        "        self.transform = transform\n",
        "        self.gray_filelist = os.listdir(self.gray_dir)\n",
        "        self.color_filelist = os.listdir(self.color_dir)\n",
        "        self.testing = testing\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.gray_filelist)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        '''\n",
        "        Takes in an index and returns both its corresponding grayscale and color images\n",
        "        '''\n",
        "        if self.testing:\n",
        "            idx = idx + 5000\n",
        "            index = str(idx)\n",
        "        else:\n",
        "            if len(str(idx)) == 1:\n",
        "                index = '000' + str(idx)\n",
        "            elif len(str(idx)) == 2:\n",
        "                index = '00' + str(idx)\n",
        "            elif len(str(idx)) == 3:\n",
        "                index = '0' + str(idx)\n",
        "            elif len(str(idx)) == 4:\n",
        "                index = str(idx)\n",
        "        gray_image = Image.open(self.gray_dir+'image' + index + '.jpg').convert(\"L\")\n",
        "        color_image = Image.open(self.color_dir+'image' + index + '.jpg').convert(\"RGB\")\n",
        "        \n",
        "        return self.transform(gray_image), self.transform(color_image)"
      ],
      "metadata": {
        "id": "rDYm9lPtQuOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset3(Dataset):\n",
        "    def __init__(self, gray_dir, color_dir, transform):\n",
        "        # gray_dir: directory of grayscale images\n",
        "        # color_dir: directory of color images\n",
        "        # transform: the possible transformation\n",
        "        self.gray_dir = gray_dir\n",
        "        self.color_dir = color_dir\n",
        "        self.transform = transform\n",
        "        self.filelist = self.make_filelist(self.gray_dir, self.color_dir)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.filelist)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        gray_filename = self.gray_dir + self.filelist[idx]\n",
        "        color_filename = self.color_dir + self.filelist[idx]\n",
        "        gray_image = Image.open(gray_filename).convert(\"L\")\n",
        "        color_image = Image.open(color_filename).convert(\"RGB\")\n",
        "        return self.transform(gray_image), self.transform(color_image)\n",
        "    \n",
        "    def make_filelist(self, gray_dir, color_dir):\n",
        "        filelist1 = []\n",
        "        for root, dirs, files in os.walk(gray_dir):\n",
        "            for file in files:\n",
        "                if file.endswith(\".jpg\"):\n",
        "                    s = os.path.join(root, file)\n",
        "                    if s.startswith(gray_dir):\n",
        "                        filelist1.append(s.replace(gray_dir, ''))\n",
        "        filelist2 = []\n",
        "        for root, dirs, files in os.walk(color_dir):\n",
        "            for file in files:\n",
        "                if file.endswith(\".jpg\"):\n",
        "                    s = os.path.join(root, file)\n",
        "                    if s.startswith(color_dir):\n",
        "                        filelist2.append(s.replace(color_dir, ''))\n",
        "        filelist = []\n",
        "        for name in filelist1:\n",
        "            if name in filelist2:\n",
        "                filelist.append(name)\n",
        "        return filelist\n"
      ],
      "metadata": {
        "id": "zicQvakS1XJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization"
      ],
      "metadata": {
        "id": "OtmfHE7cIBQY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load dataset"
      ],
      "metadata": {
        "id": "UzdEzF_bQojD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_gray_dir = \"/content/drive/Shareddrives/CVFinalProject/data/dataset1/train_black/\"\n",
        "train_color_dir = \"/content/drive/Shareddrives/CVFinalProject/data/dataset1/train_color/\"\n",
        "test_gray_dir = \"/content/drive/Shareddrives/CVFinalProject/data/dataset1/test_black/\"\n",
        "test_color_dir = \"/content/drive/Shareddrives/CVFinalProject/data/dataset1/test_color/\"\n",
        "\n",
        "output_size = (224, 224)\n",
        "batch_size = 16\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize(output_size),\n",
        "        transforms.ToTensor()\n",
        "    ]\n",
        ")\n",
        "\n",
        "train_dataset = Dataset1(train_gray_dir, train_color_dir, transform, testing=False)\n",
        "test_dataset = Dataset1(test_gray_dir, test_color_dir, transform, testing=True)\n",
        "train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
        "\n",
        "\n",
        "# train_gray_dir = \"/content/drive/Shareddrives/CVFinalProject/data/dataset3/Gray/\"\n",
        "# train_color_dir = \"/content/drive/Shareddrives/CVFinalProject/data/dataset3/ColorfulOriginal/\"\n",
        "# train_dataset = Dataset3(train_gray_dir, train_color_dir, transform)\n",
        "# train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
      ],
      "metadata": {
        "id": "JhLTCqzZQm-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(x, y) = train_dataset.__getitem__(488)"
      ],
      "metadata": {
        "id": "M94PENozQnrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using image koalarization"
      ],
      "metadata": {
        "id": "xxG-_D_rYlWf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "Y6-MIiwkZQeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Colorization(256).to(device)\n",
        "vgg_model = Vgg().to(device)"
      ],
      "metadata": {
        "id": "7UJz0-wpYkdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(\"/content/drive/Shareddrives/CVFinalProject/weights/experiment1/best_validation_model1.pth\"))"
      ],
      "metadata": {
        "id": "qnCWTruGnwkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = model(x.reshape((1,1,224,224)).to(device), vgg_model)"
      ],
      "metadata": {
        "id": "wD4Qa_-bZZ3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.imshow(torch.squeeze(x), cmap=\"gray\")\n",
        "plt.axis('off')\n",
        "plt.savefig(\"/content/drive/Shareddrives/CVFinalProject/images/original28.png\")"
      ],
      "metadata": {
        "id": "GgF6-zWZWf45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.imshow(numpy.transpose(torch.squeeze(y).numpy(), (1,2,0)))\n",
        "plt.axis(\"off\")\n",
        "plt.savefig(\"/content/drive/Shareddrives/CVFinalProject/images/groundtruth28.png\")"
      ],
      "metadata": {
        "id": "hP5MnKfDeFUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pred.shape)"
      ],
      "metadata": {
        "id": "Z5TB8NpCv3y-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.min(pred[0, 0, :, :]))"
      ],
      "metadata": {
        "id": "opIsVpRJzNaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.max(pred[0, 0, :, :]))"
      ],
      "metadata": {
        "id": "AiQ0JKRqzYaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_channels(image):\n",
        "    # shape of image should be [B, 3, H, W]\n",
        "    output = torch.zeros(image.shape)\n",
        "    for b in range(image.shape[0]):\n",
        "        for i in range(3):\n",
        "            layer_max = torch.max(image[b, i, :, :])\n",
        "            layer_min = torch.min(image[b, i, :, :])\n",
        "            output[b, i, :, :] = (image[b, i, :, :] - layer_min) / (layer_max - layer_min)\n",
        "    return output"
      ],
      "metadata": {
        "id": "qsNjavVqzhux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.squeeze(normalize_channels(pred)).shape)"
      ],
      "metadata": {
        "id": "_LsR17XY0szT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.min(pred))"
      ],
      "metadata": {
        "id": "68tkZHjr4rgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.imshow(torch.squeeze(pred).detach().cpu().permute(1,2,0), interpolation='none')\n",
        "plt.axis(\"off\")\n",
        "plt.savefig(\"/content/drive/Shareddrives/CVFinalProject/images/experiment1-deep28.png\")"
      ],
      "metadata": {
        "id": "lDUf3Spide50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unet_model = Unet().to(device)"
      ],
      "metadata": {
        "id": "ObOsnULHpCVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unet_model.load_state_dict(torch.load(\"/content/drive/Shareddrives/CVFinalProject/weights/experiment2/best_validation_model1.pth\"))"
      ],
      "metadata": {
        "id": "lkgKD03qpM0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unet_pred = unet_model(x.reshape((1,1,224,224)).to(device))"
      ],
      "metadata": {
        "id": "FBKL1EKJpVAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.imshow(numpy.transpose(torch.squeeze(unet_pred).detach().cpu().numpy(), (1,2,0)))\n",
        "plt.axis(\"off\")\n",
        "plt.savefig(\"/content/drive/Shareddrives/CVFinalProject/images/experiment2-Unet28.png\")"
      ],
      "metadata": {
        "id": "ik4wPm2tpnDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cgan = CGAN().to(device)\n",
        "cgan.load_state_dict(torch.load(\"/content/drive/Shareddrives/CVFinalProject/weights/experiment3/best_validation_model.pth\"))"
      ],
      "metadata": {
        "id": "acc_QHEBlr2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cgan_pred = cgan.net_G(x.reshape((1,1,224,224)).to(device))"
      ],
      "metadata": {
        "id": "pXVvSJ-ql8M6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.imshow(numpy.transpose(torch.squeeze(cgan_pred).detach().cpu().numpy(), (1,2,0)))\n",
        "plt.axis(\"off\")\n",
        "plt.savefig(\"/content/drive/Shareddrives/CVFinalProject/images/experiment3-CGAN28.png\")"
      ],
      "metadata": {
        "id": "5_goqmHVmKUD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}